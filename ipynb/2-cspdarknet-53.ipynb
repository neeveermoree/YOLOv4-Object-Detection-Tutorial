{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('pytorch': conda)",
   "metadata": {
    "interpreter": {
     "hash": "5a74942cef8bedd80ad2acff3fc2941fd559c84f6c215ec02fd680d9314b7eb5"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "### In this lesson we will implement CSPDarknet-53 backbone of YOLO-v4 object detection algorithm"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### In the updated version of the Yolo-v4 object recognition algorithm, CSPDarknet-53 acts as a backbone. The CSP prefix refers to the method used by Cross Stage Partial Networks. This technique allows deep neural network architectures to collect more information during gradient computation, while reducing the amount of computation performed "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "![](../img/csp1.png)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "![](../img/csp2.png)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The described significant improvements are achieved by dividing the feature map into two parts, performing calculations on one of them, and then combining the result. CSPNet contributes to a more complete and versatile gradient calculation as the gradient flows differently through both sides. The reason for this is the direct separation of the feature map, which is why a gradient with different information and weak correlation flows in different parts of the CSP layer. Thus, CSPNet makes a significant contribution to solving the following problems:\n",
    "\n",
    "    • improving the training capabilities of convolutional neural networks;\n",
    "    • reduction of downtime of computing power during operation;\n",
    "    • reducing the amount of used memory. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### With CSP in mind, let's implement the backbone before the second residual block:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "![](../img/darknet53_architecture.png)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownSample1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # implement first 8 conv layers and name them convX\n",
    "        # use Mish activation function\n",
    "        # example\n",
    "        self.conv1 = Conv_Bn_Activation(3, 32, 3, 1, 'mish')\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        input: torch.Tensor - input image data as array\n",
    "        \"\"\"\n",
    "        # from 1 to 6 conv just use conv operation\n",
    "        x1 = self.conv1(input)\n",
    "\n",
    "        # Residual block\n",
    "        # what layer to add?\n",
    "        x6 = x6\n",
    "\n",
    "        # CSP part, concatenate result with 3rd layer\n",
    "        x7 = x7\n",
    "        x8 = self.conv8(x7)\n",
    "        return x8"
   ]
  }
 ]
}