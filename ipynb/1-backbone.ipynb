{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('pytorch': conda)",
   "metadata": {
    "interpreter": {
     "hash": "5a74942cef8bedd80ad2acff3fc2941fd559c84f6c215ec02fd680d9314b7eb5"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Lesson 1\n",
    "## Backbone\n",
    "### The image below illustrates the architecture of YOLOv4 backbone:\n",
    "\n",
    "![](../img/darknet53_architecture.png)\n",
    "\n",
    "### There is the Darknet53 architecture on the left and CSP block structure on the right.\n",
    "### These architectures are from [YOLOv3: An Incremental Improvement](https://arxiv.org/pdf/1804.02767v1.pdf) and [CSPNet: A New Backbone that can Enhance Learning Capability of CNN](https://openaccess.thecvf.com/content_CVPRW_2020/papers/w28/Wang_CSPNet_A_New_Backbone_That_Can_Enhance_Learning_Capability_of_CVPRW_2020_paper.pdf) respectively.\n",
    "\n",
    "### Of course, between each Convolutinal layer Batch Normalization and Leaky ReLU (alpha=0.1) activation function are applied.\n",
    "### \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Firstly, let's code a simple block, that consists of Conv + Activation Function + Batch Norm"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "\n",
    "class Conv_Bn_Activation(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, activation, bn=True, bias=False):\n",
    "        \"\"\"\n",
    "        in_channels: int - amount of input channels (conv_parameter)\n",
    "        out_channels: int - amount of output channels (conv_parameter)\n",
    "        kernel_size: tuple or int - resolution of kernel in pixels (conv_parameter)\n",
    "        stride: int - kernel slide pixel amount (conv_parameter)\n",
    "        activation: func - function to apply non-linearity\n",
    "        bn: bool - whether to apply batchnorm\n",
    "        bias: bool - whether to add bias to convolutional operator\n",
    "        \"\"\"\n",
    "        # torch constuctor\n",
    "        super().__init__()\n",
    "\n",
    "        # you can count padding based on input parameters\n",
    "        pad = \n",
    "        \n",
    "        # add all modules to this list\n",
    "        self.conv = nn.ModuleList()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: torch.Tensor - input feature map from previous block\n",
    "        \"\"\"\n",
    "        for layer in self.conv:\n",
    "            # insert code here\n",
    "            pass\n",
    "        return x"
   ]
  },
  {
   "source": [
    "Now let's code an actual activation function used in YOLO-v4. In CSPDarknet-53 it uses Mish Activation funciton."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Its mathematical formulation looks as follows:\n",
    "\n",
    "$ f(x) = x * tanh(softplus(x)) $\n",
    "\n",
    "$ softpluts(x) = ln(1 + e^x) $\n",
    "\n",
    "Therefore, the final formula is:\n",
    "\n",
    "$ mish(x) = x * tanh(ln(1 + e^x)) $"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mish(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        # PyTorch constructor\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: torch.Tensor - data from conv block to apply non-linearity to\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        x = None\n",
    "        return x"
   ]
  }
 ]
}