{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('pytorch': conda)",
   "metadata": {
    "interpreter": {
     "hash": "5a74942cef8bedd80ad2acff3fc2941fd559c84f6c215ec02fd680d9314b7eb5"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "### In this lesson we are going to finally implement YOLO-v4 backbone part"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Let's implement additional utility class ResBlock to make project more modular"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The next helper class for the CSPDarknet-53 backbone implementation is the ResBlock class, which also inherits from the parent nn.Module.\n",
    "\n",
    "This class presents the implementation of the residual layers of the neural network, which are circled in rectangles in the Darknet-53 architecture.\n",
    "\n",
    "The main advantage of residual layers is that they can successfully deal with the problem of fading gradients.\n",
    "\n",
    "On the deep layers of the neural network, when the weights are updated, the gradient begins to tend to 0, due to which, during the back propagation of the error, the parameter weights change to extremely small values.\n",
    "\n",
    "Residual layers allow increasing the error gradient for deep layers of the neural network:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "![](../img/residual.jpg)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Mathematically:\n",
    "\n",
    "$ H(x) = F(x) + x $\n",
    "\n",
    "Where\n",
    "`F(x)` is Convolutional operator followed by activation function\n",
    "\n",
    "Also, BatchNormalization can be applied in F(x)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "This class takes as arguments the number of input and output channels and the number of consecutive residual blocks.\n",
    "\n",
    "The implementation of the ResBlock class is below:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Sequential residual blocks each of which consists of \\\n",
    "    two convolution layers.\n",
    "    Args:\n",
    "        ch (int): number of input and output channels.\n",
    "        nblocks (int): number of residual blocks.\n",
    "        shortcut (bool): if True, residual tensor addition is enabled.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ch, nblocks=1, shortcut=True):\n",
    "        super().__init__()\n",
    "        self.shortcut = shortcut\n",
    "        self.module_list = nn.ModuleList()\n",
    "        # Use Mish activation function\n",
    "        for i in range(nblocks):\n",
    "            resblock_one = nn.ModuleList()\n",
    "            # store blocks into resblock_one variable\n",
    "            # YOUR CODE HERE\n",
    "            self.module_list.append(resblock_one)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for module in self.module_list:\n",
    "            h = x\n",
    "            for res in module:\n",
    "                # define h here as the output of residual blocks\n",
    "                pass\n",
    "            x = x + h if self.shortcut else h\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}